{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BERT_v1.0_new.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yidail/IDL-Final-Project/blob/main/BERT_v1_0_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIv-pXIZO_Um",
        "outputId": "f967444b-1a97-45dd-e337-4c4c1f1825f1"
      },
      "source": [
        "!nvidia-smi -Lâ€©"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla V100-SXM2-16GB (UUID: GPU-98e78472-d28a-da39-68ce-265b554019d5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KRkMTmm0XGZ",
        "outputId": "46bf9181-8ad8-4c12-c7ed-52020ff6fc60"
      },
      "source": [
        "# connect to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDJmctfi02Ro"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9NlU_jy04en",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f060d263-5cea-4d85-f30d-18491900e30d"
      },
      "source": [
        "%cd '/content/gdrive/MyDrive/BERT_new'\n",
        "!ls"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/BERT_new\n",
            "BERT_v1.0_new.ipynb  evaluate-v2.0.py\t    run_qa.py\t     trainer_qa.py\n",
            "dev-v1.1.json\t     __pycache__\t    runs\t     train-v1.1.json\n",
            "evaluate-v1.1.py     run_qa_beam_search.py  run_tf_squad.py  utils_qa.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4zRmLvkf8Ze"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0B4t25f6bD0",
        "outputId": "3701efb5-ad1c-43f1-8df4-2548ef1f91fc"
      },
      "source": [
        "!python run_qa.py \\\n",
        "  --model_name_or_path distilbert-base-uncased \\\n",
        "  --dataset_name squad \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --per_device_train_batch_size 12 \\\n",
        "  --learning_rate 3e-5 \\\n",
        "  --num_train_epochs 1 \\\n",
        "  --max_seq_length 384 \\\n",
        "  --doc_stride 128 \\\n",
        "  --overwrite_output_dir \\\n",
        "  --output_dir /tmp/debug_squad/"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-09 00:26:40.746419: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "12/09/2020 00:26:44 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "12/09/2020 00:26:44 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/debug_squad/', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=12, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=3e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Dec09_00-26-44_e16a085d0486', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/debug_squad/', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n",
            "Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41)\n",
            "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361\n",
            "Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361\n",
            "Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBert_CNN: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBert_CNN from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBert_CNN from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBert_CNN were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41/cache-1be805fef08d0ae3.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41/cache-a6bde4e398111bff.arrow\n",
            "The following columns in the training set don't have a corresponding argument in `DistilBert_CNN.forward` and have been ignored: .\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBert_CNN.forward` and have been ignored: offset_mapping, example_id.\n",
            "***** Running training *****\n",
            "  Num examples = 88524\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 12\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 7377\n",
            "{'loss': 2.709924072265625, 'learning_rate': 2.796665311102074e-05, 'epoch': 0.06777822963264199}\n",
            "  7% 500/7377 [01:22<17:54,  6.40it/s]Saving model checkpoint to /tmp/debug_squad/checkpoint-500\n",
            "Configuration saved in /tmp/debug_squad/checkpoint-500/config.json\n",
            "Model weights saved in /tmp/debug_squad/checkpoint-500/pytorch_model.bin\n",
            "{'loss': 1.7083270263671875, 'learning_rate': 2.593330622204148e-05, 'epoch': 0.13555645926528398}\n",
            " 14% 1000/7377 [02:44<16:32,  6.42it/s]Saving model checkpoint to /tmp/debug_squad/checkpoint-1000\n",
            "Configuration saved in /tmp/debug_squad/checkpoint-1000/config.json\n",
            "Model weights saved in /tmp/debug_squad/checkpoint-1000/pytorch_model.bin\n",
            "{'loss': 1.613985107421875, 'learning_rate': 2.389995933306222e-05, 'epoch': 0.203334688897926}\n",
            " 20% 1500/7377 [04:05<15:01,  6.52it/s]Saving model checkpoint to /tmp/debug_squad/checkpoint-1500\n",
            "Configuration saved in /tmp/debug_squad/checkpoint-1500/config.json\n",
            "Model weights saved in /tmp/debug_squad/checkpoint-1500/pytorch_model.bin\n",
            "{'loss': 1.4819110107421876, 'learning_rate': 2.1866612444082963e-05, 'epoch': 0.27111291853056796}\n",
            " 27% 2000/7377 [05:25<13:46,  6.50it/s]Saving model checkpoint to /tmp/debug_squad/checkpoint-2000\n",
            "Configuration saved in /tmp/debug_squad/checkpoint-2000/config.json\n",
            "Model weights saved in /tmp/debug_squad/checkpoint-2000/pytorch_model.bin\n",
            "{'loss': 1.4213748779296875, 'learning_rate': 1.9833265555103702e-05, 'epoch': 0.33889114816321}\n",
            " 34% 2500/7377 [06:45<12:28,  6.51it/s]Saving model checkpoint to /tmp/debug_squad/checkpoint-2500\n",
            "Configuration saved in /tmp/debug_squad/checkpoint-2500/config.json\n",
            "Model weights saved in /tmp/debug_squad/checkpoint-2500/pytorch_model.bin\n",
            "{'loss': 1.349160400390625, 'learning_rate': 1.779991866612444e-05, 'epoch': 0.406669377795852}\n",
            " 41% 3000/7377 [08:04<11:10,  6.52it/s]Saving model checkpoint to /tmp/debug_squad/checkpoint-3000\n",
            "Configuration saved in /tmp/debug_squad/checkpoint-3000/config.json\n",
            "Model weights saved in /tmp/debug_squad/checkpoint-3000/pytorch_model.bin\n",
            "{'loss': 1.3591448974609375, 'learning_rate': 1.576657177714518e-05, 'epoch': 0.47444760742849396}\n",
            " 47% 3500/7377 [09:24<09:52,  6.54it/s]Saving model checkpoint to /tmp/debug_squad/checkpoint-3500\n",
            "Configuration saved in /tmp/debug_squad/checkpoint-3500/config.json\n",
            "Model weights saved in /tmp/debug_squad/checkpoint-3500/pytorch_model.bin\n",
            "{'loss': 1.3057366943359374, 'learning_rate': 1.373322488816592e-05, 'epoch': 0.5422258370611359}\n",
            " 54% 4000/7377 [10:45<08:37,  6.52it/s]Saving model checkpoint to /tmp/debug_squad/checkpoint-4000\n",
            "Configuration saved in /tmp/debug_squad/checkpoint-4000/config.json\n",
            "Model weights saved in /tmp/debug_squad/checkpoint-4000/pytorch_model.bin\n",
            "{'loss': 1.26896728515625, 'learning_rate': 1.1699877999186661e-05, 'epoch': 0.6100040666937779}\n",
            " 61% 4500/7377 [12:05<07:26,  6.44it/s]Saving model checkpoint to /tmp/debug_squad/checkpoint-4500\n",
            "Configuration saved in /tmp/debug_squad/checkpoint-4500/config.json\n",
            "Model weights saved in /tmp/debug_squad/checkpoint-4500/pytorch_model.bin\n",
            "{'loss': 1.24832275390625, 'learning_rate': 9.666531110207402e-06, 'epoch': 0.67778229632642}\n",
            " 68% 5000/7377 [13:25<06:13,  6.36it/s]Saving model checkpoint to /tmp/debug_squad/checkpoint-5000\n",
            "Configuration saved in /tmp/debug_squad/checkpoint-5000/config.json\n",
            "Model weights saved in /tmp/debug_squad/checkpoint-5000/pytorch_model.bin\n",
            "{'loss': 1.2207681884765624, 'learning_rate': 7.633184221228141e-06, 'epoch': 0.745560525959062}\n",
            " 75% 5500/7377 [14:45<04:46,  6.55it/s]Saving model checkpoint to /tmp/debug_squad/checkpoint-5500\n",
            "Configuration saved in /tmp/debug_squad/checkpoint-5500/config.json\n",
            "Model weights saved in /tmp/debug_squad/checkpoint-5500/pytorch_model.bin\n",
            "{'loss': 1.2341162109375, 'learning_rate': 5.5998373322488825e-06, 'epoch': 0.813338755591704}\n",
            " 81% 6000/7377 [16:05<03:30,  6.53it/s]Saving model checkpoint to /tmp/debug_squad/checkpoint-6000\n",
            "Configuration saved in /tmp/debug_squad/checkpoint-6000/config.json\n",
            "Model weights saved in /tmp/debug_squad/checkpoint-6000/pytorch_model.bin\n",
            "{'loss': 1.2237156982421875, 'learning_rate': 3.566490443269622e-06, 'epoch': 0.8811169852243459}\n",
            " 88% 6500/7377 [17:25<02:16,  6.42it/s]Saving model checkpoint to /tmp/debug_squad/checkpoint-6500\n",
            "Configuration saved in /tmp/debug_squad/checkpoint-6500/config.json\n",
            "Model weights saved in /tmp/debug_squad/checkpoint-6500/pytorch_model.bin\n",
            "{'loss': 1.172209716796875, 'learning_rate': 1.533143554290362e-06, 'epoch': 0.9488952148569879}\n",
            " 95% 7000/7377 [18:45<00:58,  6.42it/s]Saving model checkpoint to /tmp/debug_squad/checkpoint-7000\n",
            "Configuration saved in /tmp/debug_squad/checkpoint-7000/config.json\n",
            "Model weights saved in /tmp/debug_squad/checkpoint-7000/pytorch_model.bin\n",
            "100% 7377/7377 [19:47<00:00,  6.50it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'epoch': 1.0}\n",
            "100% 7377/7377 [19:47<00:00,  6.21it/s]\n",
            "Saving model checkpoint to /tmp/debug_squad/\n",
            "Configuration saved in /tmp/debug_squad/config.json\n",
            "Model weights saved in /tmp/debug_squad/pytorch_model.bin\n",
            "12/09/2020 00:46:48 - INFO - __main__ -   *** Evaluate ***\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBert_CNN.forward` and have been ignored: offset_mapping, example_id.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10784\n",
            "  Batch size = 8\n",
            "100% 1345/1348 [00:38<00:00, 35.53it/s]12/09/2020 00:47:35 - INFO - utils_qa -   Post-processing 10570 example predictions split into 10784 features.\n",
            "100% 10570/10570 [00:22<00:00, 479.15it/s]\n",
            "12/09/2020 00:47:57 - INFO - utils_qa -   Saving predictions to /tmp/debug_squad/predictions.json.\n",
            "12/09/2020 00:47:57 - INFO - utils_qa -   Saving nbest_preds to /tmp/debug_squad/nbest_predictions.json.\n",
            "100% 1348/1348 [01:13<00:00, 18.41it/s]\n",
            "12/09/2020 00:48:02 - INFO - __main__ -   ***** Eval results *****\n",
            "12/09/2020 00:48:02 - INFO - __main__ -     exact_match = 75.06149479659413\n",
            "12/09/2020 00:48:02 - INFO - __main__ -     f1 = 83.8120724715015\n",
            "12/09/2020 00:48:02 - INFO - __main__ -     epoch = 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}