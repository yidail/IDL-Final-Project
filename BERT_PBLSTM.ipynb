{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"BERT_viola.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BIv-pXIZO_Um","executionInfo":{"status":"ok","timestamp":1607478839508,"user_tz":480,"elapsed":728,"user":{"displayName":"Viola S.","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZYMlOTWfFtKqx8bjt1WL1dy26D9IH4x1DEADp=s64","userId":"06799168636695050943"}},"outputId":"ee898fd3-bc35-49eb-ea59-cfb0ff87b0f5"},"source":["!nvidia-smi -Lâ€©"],"execution_count":1,"outputs":[{"output_type":"stream","text":["GPU 0: Tesla V100-SXM2-16GB (UUID: GPU-95b2c041-91ce-e912-792b-f083c5323ec8)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7KRkMTmm0XGZ","executionInfo":{"status":"ok","timestamp":1607478854112,"user_tz":480,"elapsed":15322,"user":{"displayName":"Viola S.","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZYMlOTWfFtKqx8bjt1WL1dy26D9IH4x1DEADp=s64","userId":"06799168636695050943"}},"outputId":"d09c1f4d-9540-410c-c788-c07c4a57539b"},"source":["# connect to Google Drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eDJmctfi02Ro"},"source":["!pip install transformers\n","!pip install datasets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z9NlU_jy04en","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607480187301,"user_tz":480,"elapsed":806,"user":{"displayName":"Viola S.","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZYMlOTWfFtKqx8bjt1WL1dy26D9IH4x1DEADp=s64","userId":"06799168636695050943"}},"outputId":"99f1b758-9763-406e-d946-857a9cac0c13"},"source":["%cd '/content/gdrive/MyDrive/BERT_copy'\n","!ls"],"execution_count":40,"outputs":[{"output_type":"stream","text":["/content/gdrive/MyDrive/BERT_copy\n","BERT_architecture.py  evaluate-v2.0.py\t     runs\t      train-v1.1.json\n","BERT_viola.ipynb      __pycache__\t     run_tf_squad.py  utils_qa.py\n","dev-v1.1.json\t      run_qa_beam_search.py  squad_v2_local\n","evaluate-v1.1.py      run_qa.py\t\t     trainer_qa.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d0B4t25f6bD0"},"source":["# # run squad1.1\n","# !python run_qa.py \\\n","#   --model_name_or_path distilbert-base-uncased \\\n","#   --dataset_name squad \\\n","#   --do_train \\\n","#   --do_eval \\\n","#   --per_device_train_batch_size 12 \\\n","#   --learning_rate 3e-5 \\\n","#   --num_train_epochs 1 \\\n","#   --max_seq_length 384 \\\n","#   --doc_stride 128 \\\n","#   --overwrite_output_dir \\\n","#   --output_dir /tmp/debug_squad/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0BjL_wA_NURX","outputId":"592fc7b5-7d3d-4f55-b37b-91949cbec786"},"source":["# run squad2.0\n","!python run_qa.py \\\n","  --model_name_or_path distilbert-base-uncased \\\n","  --dataset_name squad_v2 \\\n","  --version_2_with_negative \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 24 \\\n","  --learning_rate 3e-5 \\\n","  --num_train_epochs 2 \\\n","  --max_seq_length 384 \\\n","  --doc_stride 128 \\\n","  --overwrite_output_dir \\\n","  --output_dir /tmp/debug_squad/ \\\n","  --save_steps 2000"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2020-12-09 02:30:02.711042: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n","12/09/2020 02:30:04 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","12/09/2020 02:30:04 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/debug_squad/', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=24, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=3e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Dec09_02-30-04_badd818bdb9e', logging_first_step=False, logging_steps=500, save_steps=2000, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/debug_squad/', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n","Reusing dataset squad_v2 (/root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/0e44b51f4035c15e218d53dc9eea5fe7123341982e524818b8500e4094fffb7b)\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361\n","Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361\n","Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBert_pblstm: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBert_pblstm from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBert_pblstm from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBert_pblstm were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['blstm.weight_ih_l0', 'blstm.weight_hh_l0', 'blstm.bias_ih_l0', 'blstm.bias_hh_l0', 'blstm.weight_ih_l0_reverse', 'blstm.weight_hh_l0_reverse', 'blstm.bias_ih_l0_reverse', 'blstm.bias_hh_l0_reverse', 'pblstm.blstm.weight_ih_l0', 'pblstm.blstm.weight_hh_l0', 'pblstm.blstm.bias_ih_l0', 'pblstm.blstm.bias_hh_l0', 'pblstm.blstm.weight_ih_l0_reverse', 'pblstm.blstm.weight_hh_l0_reverse', 'pblstm.blstm.bias_ih_l0_reverse', 'pblstm.blstm.bias_hh_l0_reverse', 'qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/0e44b51f4035c15e218d53dc9eea5fe7123341982e524818b8500e4094fffb7b/cache-bf09c3fec10e89a5.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/0e44b51f4035c15e218d53dc9eea5fe7123341982e524818b8500e4094fffb7b/cache-94917a406addca3e.arrow\n","The following columns in the training set don't have a corresponding argument in `DistilBert_pblstm.forward` and have been ignored: .\n","The following columns in the evaluation set don't have a corresponding argument in `DistilBert_pblstm.forward` and have been ignored: example_id, offset_mapping.\n","***** Running training *****\n","  Num examples = 131754\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 24\n","  Total train batch size (w. parallel, distributed & accumulation) = 24\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 10980\n","  4% 430/10980 [02:33<1:02:57,  2.79it/s]"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_RSZBQ9not-M","executionInfo":{"status":"ok","timestamp":1607479741955,"user_tz":480,"elapsed":3028,"user":{"displayName":"Viola S.","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZYMlOTWfFtKqx8bjt1WL1dy26D9IH4x1DEADp=s64","userId":"06799168636695050943"}}},"source":["import math\n","import os\n","import warnings\n","from dataclasses import dataclass\n","from typing import Optional, Tuple\n","\n","import torch\n","import torch.utils.checkpoint\n","from torch import nn\n","from torch.nn import CrossEntropyLoss, MSELoss\n","\n","from transformers import BertTokenizer, BertModel, BertPreTrainedModel\n","from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer, DistilBertPreTrainedModel\n","\n","from transformers.modeling_outputs import (\n","    BaseModelOutputWithCrossAttentions,\n","    BaseModelOutputWithPoolingAndCrossAttentions,\n","    CausalLMOutputWithCrossAttentions,\n","    MaskedLMOutput,\n","    MultipleChoiceModelOutput,\n","    NextSentencePredictorOutput,\n","    QuestionAnsweringModelOutput,\n","    SequenceClassifierOutput,\n","    TokenClassifierOutput,\n",")\n","class pBLSTM(nn.Module):\n","    '''\n","    Pyramidal BiLSTM\n","    The length of utterance (speech input) can be hundereds to thousands of frames long.\n","    The Paper reports that a direct LSTM implementation as Encoder resulted in slow convergence,\n","    and inferior results even after extensive training.\n","    The major reason is inability of AttendAndSpell operation to extract relevant information\n","    from a large number of input steps.\n","    '''\n","    def __init__(self, input_dim, hidden_dim):\n","        super(pBLSTM, self).__init__()\n","        self.blstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n","\n","    def forward(self, seq_unpacked):\n","        '''\n","        :param x :(N, T) input to the pBLSTM\n","        :return output: (N, T, H) encoded sequence from pyramidal Bi-LSTM \n","        '''\n","\n","        # seq_unpacked, lens_unpacked = utils.rnn.pad_packed_sequence(x, batch_first=True)\n","\n","        if seq_unpacked.shape[1] % 2 == 1:\n","            seq_unpacked = seq_unpacked[:, :-1, :]\n","\n","        x = seq_unpacked.reshape(seq_unpacked.shape[0], seq_unpacked.shape[1]//2, seq_unpacked.shape[2]*2)\n","\n","        outputs, _ = self.blstm(x)\n","\n","        return outputs"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"CYkGH3YL4a9I"},"source":["# attempt 1 of writing our model\n","import math\n","import os\n","import warnings\n","from dataclasses import dataclass\n","from typing import Optional, Tuple\n","\n","import torch\n","import torch.utils.checkpoint\n","from torch import nn\n","from torch.nn import CrossEntropyLoss, MSELoss\n","\n","from transformers import BertTokenizer, BertModel, BertPreTrainedModel\n","\n","from transformers.modeling_outputs import (\n","    BaseModelOutputWithCrossAttentions,\n","    BaseModelOutputWithPoolingAndCrossAttentions,\n","    CausalLMOutputWithCrossAttentions,\n","    MaskedLMOutput,\n","    MultipleChoiceModelOutput,\n","    NextSentencePredictorOutput,\n","    QuestionAnsweringModelOutput,\n","    SequenceClassifierOutput,\n","    TokenClassifierOutput,\n",")\n","\n","\n","class BERT_CNN(BertPreTrainedModel):\n","    '''\n","    ATTEMPT NO. 1\n","    '''\n","    def __init__(self, config):\n","        # super(BERT_CNN, self).__init__()\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        # layer1: BERT\n","        # pretrained_weights='bert-base-uncased'\n","        # self.tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n","        self.bert = BertModel(config, add_pooling_layer=False)\n","\n","        # layer2: LSTM - according to the Stanford paper: https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/reports/default/15848021.pdf\n","        hidden_dim = 256 # user defined\n","        input_dim = config.hidden_size # warning! might need to change, 768 = sequence_output.shape[2]\n","        self.lstm = nn.LSTM(input_size=config.hidden_size, hidden_size=hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n","        \n","        # layer3: CNN\n","        # WARNING! CNN dim's not yet working \n","        # self.cnn = nn.Conv2d(in_channels=1, out_channels=hidden_cnn, kernel_size=3)\n","\n","        # layer4: Linear Output Layer\n","        # use this if you want the cnn layer\n","        # self.qa_outputs = nn.Linear(hidden_cnn, vocab_size)\n","        # use this if omit the cnn layer\n","        self.qa_outputs = nn.Linear(2*hidden_dim, config.num_labels)\n","        self.init_weights()\n","\n","    # WARNING! not sure if it would work for text in a batch \n","    def forward(self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        start_positions=None,\n","        end_positions=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,):\n","      \n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        sequence_output = outputs[0]\n","        # 1: encode text and send to bert layer\n","        # bert_output shape: (batch_size, sequence_length, hidden_size)\n","        # encoded_input = self.tokenizer(text, return_tensors='pt')\n","        # output = self.model(**encoded_input)\n","        # bert_output = output[0]\n","\n","        # 2: lstm layer\n","        # input of shape (batch, seq_len, input_size)\n","        # output from lstm has size (batch, seq_len, 2*input_size)\n","        lstm_output, _ = self.lstm(sequence_output)\n","\n","        # 3: cnn layer\n","        # cnn input (N, Cin, Hin, Win)\n","        # cnn input (N, Cout, Hout, Wout)\n","        # not sure abou the dimensions yet\n","        # cnn_input = lstm_output.unsqueeze(1) # did this according to the Stanford paper, need more investigation\n","        # cnn_output = cnn(cnn_input)\n","\n","        # final linear layer\n","        # WARNING! I assume we output text using argmax (greedy search)\n","        # maybe we only need the logits to calculate F1 score\n","        logits = self.qa_outputs(lstm_output)\n","        # vocab_index = logits.argmax(dim=2)\n","\n","        # found this piece of code from BERT database: https://huggingface.co/transformers/_modules/transformers/modeling_bert.html#BertForTokenClassification\n","        # not sure why they added those\n","        start_logits, end_logits = logits.split(1, dim=-1)\n","        start_logits = start_logits.squeeze(-1)\n","        end_logits = end_logits.squeeze(-1)\n","\n","        total_loss = None\n","        if start_positions is not None and end_positions is not None:\n","            # If we are on multi-GPU, split add a dimension\n","            if len(start_positions.size()) > 1:\n","                start_positions = start_positions.squeeze(-1)\n","            if len(end_positions.size()) > 1:\n","                end_positions = end_positions.squeeze(-1)\n","            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n","            ignored_index = start_logits.size(1)\n","            start_positions.clamp_(0, ignored_index)\n","            end_positions.clamp_(0, ignored_index)\n","\n","            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n","            start_loss = loss_fct(start_logits, start_positions)\n","            end_loss = loss_fct(end_logits, end_positions)\n","            total_loss = (start_loss + end_loss) / 2\n","\n","        if not return_dict:\n","            output = (start_logits, end_logits) + outputs[2:]\n","            return ((total_loss,) + output) if total_loss is not None else output\n","\n","        return QuestionAnsweringModelOutput(\n","            loss=total_loss,\n","            start_logits=start_logits,\n","            end_logits=end_logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )\n","\n","        # ultimately we want to return a loss\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZsftadkdQlBY"},"source":["self.distilbert = DistilBertModel(config)\n","self.blstm = nn.LSTM(input_size=config.dim, hidden_size=256, num_layers=1, bidirectional=True, batch_first=True)\n","self.dropout = nn.Dropout(config.qa_dropout)\n","self.pblstm = pBLSTM(input_dim=256*4, hidden_dim=256)\n","self.qa_outputs = nn.Linear(256*4, config.num_labels)\n","\n","assert config.num_labels == 2\n","self.init_weights()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"at_OXLbdpcqT","executionInfo":{"status":"ok","timestamp":1607480157109,"user_tz":480,"elapsed":4415,"user":{"displayName":"Viola S.","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZYMlOTWfFtKqx8bjt1WL1dy26D9IH4x1DEADp=s64","userId":"06799168636695050943"}}},"source":["\n","\n","pretrained_weights='bert-base-uncased'\n","tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n","model = BertModel.from_pretrained(pretrained_weights)\n","\n","blstm = nn.LSTM(input_size=768, hidden_size=256, num_layers=1, bidirectional=True, batch_first=True)\n","dropout = nn.Dropout(0.5)\n","pblstm = pBLSTM(input_dim=256*4, hidden_dim=256)\n","qa_outputs = nn.Linear(256*2, 3000)\n","\n","text = \"let us try to see what it outputs\"\n","encoded_input = tokenizer(text, return_tensors='pt')\n","output = model(**encoded_input)\n","\n","hidden_states = output[0] # last hidden state\n","\n","lstm_output, _ = blstm(hidden_states)\n","hidden_states = dropout(lstm_output)  # (bs, max_query_len, dim)\n","hidden_states = pblstm(hidden_states)\n","logits = qa_outputs(hidden_states)  # (bs, max_query_len, 2)\n","# # lstm layer\n","# # input of shape (batch, seq_len, input_size)\n","# # output from lstm has size torch.Size([1, 5, 512])\n","# hidden_dim = 256\n","# input_dim = sequence_output.shape[2]\n","# lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n","# lstm_output, _ = lstm(sequence_output)\n","\n","# # # cnn input (N, Cin, Hin, Win)\n","# # # cnn input (N, Cout, Hout, Wout)\n","# # # not sure yet\n","# # cnn_input = lstm_output.unsqueeze(1) \n","# # hidden_cnn = 512\n","# # cnn = nn.Conv2d(in_channels=1, out_channels=hidden_cnn, kernel_size=3)\n","# # cnn_output = cnn(cnn_input)\n","\n","# # final linear layer\n","# vocab_size = 30522\n","# qa_outputs = nn.Linear(2*hidden_dim, vocab_size)\n","# logits = qa_outputs(lstm_output)\n","# # get max? Assuming greedy search\n","# print(logits.argmax(dim=2))\n","# # start_logits, end_logits = logits.split(1, dim=-1)\n","# # start_logits = start_logits.squeeze(-1)\n","# # end_logits = end_logits.squeeze(-1)"],"execution_count":39,"outputs":[]}]}
